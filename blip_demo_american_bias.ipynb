{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2b949f9f",
      "metadata": {
        "id": "2b949f9f"
      },
      "source": [
        "# BLIP: Inference Demo\n",
        " - [Image Captioning](#Image-Captioning)\n",
        " - [VQA](#VQA)\n",
        " - [Feature Extraction](#Feature-Extraction)\n",
        " - [Image Text Matching](#Image-Text-Matching)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbcb066b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcb066b",
        "outputId": "dae777e0-e1ae-421c-f586-8d654fb7c1ad"
      },
      "outputs": [],
      "source": [
        "#Based on the publicly available demo for BLIP: https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb#scrollTo=a811a65f\n",
        "#To replicate with this notebook, you need to run from the above demo link.\n",
        "\n",
        "# install requirements\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a811a65f",
      "metadata": {
        "id": "a811a65f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Default code from BLIP demo\n",
        "def load_demo_image(image_size,device):\n",
        "    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
        "    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')   \n",
        "\n",
        "    w,h = raw_image.size\n",
        "    display(raw_image.resize((w//5,h//5)))\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
        "    return image\n",
        "\n",
        "#Added a function to load and preprocess images\n",
        "def load_image(img_url,image_size,device):\n",
        "    raw_image = Image.open(img_url).convert('RGB')   \n",
        "    w,h = raw_image.size\n",
        "    display(raw_image.resize((w//5,h//5)))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YXgvdA7R4hMZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXgvdA7R4hMZ",
        "outputId": "8ffd9a9f-8877-4a70-a80b-b9f993390852"
      },
      "outputs": [],
      "source": [
        "#Import Drive to read in CFD images and subsequently write data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6835daef",
      "metadata": {
        "id": "6835daef",
        "outputId": "1277f34c-e77c-4fa5-81e5-300f58421697"
      },
      "outputs": [],
      "source": [
        "from models.blip import blip_decoder\n",
        "\n",
        "image_size = 384\n",
        "image = load_demo_image(image_size=image_size, device=device)\n",
        "\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'\n",
        "\n",
        "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "#Image source\n",
        "IMG_SOURCE = f'/content/BLIP/cfd/'\n",
        "images = listdir('/content/BLIP/cfd/')\n",
        "\n",
        "caption_string = ''\n",
        "\n",
        "for img in images:\n",
        "  image = load_image(f'{IMG_SOURCE}{img}',image_size,device)\n",
        "  with torch.no_grad():\n",
        "    for p in [0,.1,.2,.3,.4,.5,.6,.7,.8,.9]:\n",
        "      caption = model.generate(image, sample=True, top_p=p, max_length=20, min_length=5)[0]\n",
        "      caption_string += f'{img}\\t{p}\\t{caption}'\n",
        "\n",
        "with open(f'/content/drive/My Drive/blip_results/blip_caption_string.txt','w') as blip_writer:\n",
        "  blip_writer.write(caption_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6f3fb1",
      "metadata": {
        "id": "5e6f3fb1",
        "outputId": "cf3f98ad-0464-42bf-a295-834ac2441737"
      },
      "outputs": [],
      "source": [
        "#Visual Question Answering Tasks using BLIP VQA\n",
        "from models.blip_vqa import blip_vqa\n",
        "from os import listdir\n",
        "\n",
        "#Model Parameters\n",
        "image_size = 480\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_vqa.pth'\n",
        "    \n",
        "#Model Initialization\n",
        "model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "#Image source\n",
        "IMG_SOURCE = f'/content/BLIP/cfd/'\n",
        "images = listdir('/content/BLIP/cfd/')\n",
        "\n",
        "#American Question\n",
        "caption_string = ''\n",
        "\n",
        "question = 'is this person an american?'\n",
        "\n",
        "for img in images:\n",
        "  image = load_image(f'{IMG_SOURCE}{img}',image_size,device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    answer = model(image, question, train=False, inference='generate') \n",
        "    caption_string += f'{img} :: {answer[0]}\\n'\n",
        "\n",
        "with open(f'/content/drive/My Drive/blip_results/blip_american_string.txt','w') as blip_writer:\n",
        "  blip_writer.write(caption_string)\n",
        "\n",
        "#State Question\n",
        "caption_string = ''\n",
        "\n",
        "question = 'what state does this person live in?'\n",
        "\n",
        "for img in images:\n",
        "  image = load_image(f'{IMG_SOURCE}{img}',image_size,device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    answer = model(image, question, train=False, inference='generate') \n",
        "    caption_string += f'{img} :: {answer[0]}\\n'\n",
        "\n",
        "with open(f'/content/drive/My Drive/blip_results/blip_state_string.txt','w') as blip_writer:\n",
        "  blip_writer.write(caption_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a3f73d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Strings for Text Embeddings\n",
        "states_dict = {\n",
        "    'Alaska':'ak',\n",
        "    'Alabama':'al',\n",
        "    'Arkansas':'ar',\n",
        "    'Arizona':'az',\n",
        "    'California':'ca',\n",
        "    'Colorado':'co',\n",
        "    'Connecticut':'ct',\n",
        "    'Delaware':'de',\n",
        "    'Florida':'fl',\n",
        "    'Georgia':'ga',\n",
        "    'Hawaii':'hi',\n",
        "    'Iowa':'ia',\n",
        "    'Idaho':'id',\n",
        "    'Indiana':'in',\n",
        "    'Illinois':'il',\n",
        "    'Kansas':'ks',\n",
        "    'Kentucky':'ky',\n",
        "    'Louisiana':'la',\n",
        "    'Maine':'me',\n",
        "    'Massachusetts':'ma',\n",
        "    'Maryland':'md',\n",
        "    'Michigan':'mi',\n",
        "    'Minnesota':'mn',\n",
        "    'Missouri':'mo',\n",
        "    'Mississippi':'ms',\n",
        "    'Montana':'mt',\n",
        "    'North Carolina':'nc',\n",
        "    'Nebraska':'ne',\n",
        "    'New Hampshire':'nh',\n",
        "    'New Jersey':'nj',\n",
        "    'New Mexico':'nm',\n",
        "    'Nevada':'nv',\n",
        "    'New York':'ny',\n",
        "    'North Dakota':'nd',\n",
        "    'Ohio':'oh',\n",
        "    'Oklahoma':'ok',\n",
        "    'Oregon':'or',\n",
        "    'Pennsylvania':'pa',\n",
        "    'Rhode Island':'ri',\n",
        "    'South Carolina':'sc',\n",
        "    'South Dakota':'sd',\n",
        "    'Tennessee':'tn',\n",
        "    'Texas':'tx',\n",
        "    'Utah':'ut',\n",
        "    'Virginia':'va',\n",
        "    'Vermont':'vt',\n",
        "    'Washington':'wa',\n",
        "    'West Virginia':'wv',\n",
        "    'Wisconsin':'wi',\n",
        "    'Wyoming':'wy',\n",
        "    'District of Columbia':'dc',\n",
        "}\n",
        "\n",
        "#We/They WEAT\n",
        "american_stimuli = ['we','us','our','ours','ourselves','familiar','similar','here']\n",
        "foreign_stimuli = ['they','them','their','theirs','themselves','other','others','there']\n",
        "\n",
        "#Defining American Target Stimuli\n",
        "defining_american_targets = [f'a photo of someone who is patriotic',f'a photo of someone who is an immigrant to America',f'a photo of someone who treats people of all races and backgrounds equally']\n",
        "\n",
        "#State Stimuli\n",
        "states = list(states_dict.keys())\n",
        "state_someone = [f'a photo of someone who lives in the state of {state}' for state in states] + [f'a photo of someone who lives in Washington, D.C.']\n",
        "\n",
        "all_language_targets = american_stimuli + foreign_stimuli + defining_american_targets + states + state_someone\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ba5906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "49ba5906",
        "outputId": "2b10c9b3-cac0-418a-b4c4-4e99ed114f8a"
      },
      "outputs": [],
      "source": [
        "#Image Text Matching with BLIP ITM embeddings\n",
        "from models.blip_itm import blip_itm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "\n",
        "#Standard image preprocessing\n",
        "def process_image(image,image_size,device):\n",
        "    raw_image = Image.open(image).convert('RGB')   \n",
        "\n",
        "    w,h = raw_image.size\n",
        "    display(raw_image.resize((w//5,h//5)))\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
        "    return image\n",
        "\n",
        "#Function to get image embedding\n",
        "def get_image_embedding(image):\n",
        "  with torch.no_grad():\n",
        "    image_embeds = model.visual_encoder(image)\n",
        "    image_feat = model.vision_proj(image_embeds[:,0,:]).numpy().squeeze()\n",
        "  return image_feat\n",
        "\n",
        "#Function to get text embedding\n",
        "def get_text_embedding(text):\n",
        "  with torch.no_grad():\n",
        "      text = model.tokenizer(caption, padding='max_length', truncation=True, max_length=35, \n",
        "                              return_tensors=\"pt\").to(image.device)\n",
        "\n",
        "      text_embeds = model.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n",
        "                                            return_dict = True, mode = 'text')\n",
        "      text_feat = model.text_proj(text_embeds.last_hidden_state[:,0,:]).numpy().squeeze()\n",
        "\n",
        "  return text_feat\n",
        "\n",
        "#Model Parameters\n",
        "image_size = 384\n",
        "MODEL_ = 'blip_itm'\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
        "    \n",
        "#Model Initialization\n",
        "model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "#Get CFD Embeddings\n",
        "image_source = f'/content/drive/My Drive/CFD'\n",
        "image_list = listdir(image_source)\n",
        "\n",
        "img_embs = []\n",
        "\n",
        "for image in image_list:\n",
        "  processed_img = process_image(image,image_size,device)\n",
        "  img_emb = get_image_embedding(processed_img)\n",
        "  img_embs.append(img_emb)\n",
        "\n",
        "emb_arr = np.array(img_embs)\n",
        "emb_df = pd.DataFrame(emb_arr,index=image_list)\n",
        "emb_df.to_csv(f'/content/drive/My Drive/blip_embeddings/embedding_df_{MODEL_}.vec',sep = ' ')\n",
        "\n",
        "#Get text embeddings\n",
        "text_embs = []\n",
        "\n",
        "for phrase in all_language_targets:\n",
        "  text_emb = get_text_embedding(phrase)\n",
        "  text_embs.append(text_emb)\n",
        "\n",
        "emb_arr = np.array(text_embs)\n",
        "emb_df = pd.DataFrame(emb_arr,index=all_language_targets)\n",
        "\n",
        "emb_df.to_csv(f'/content/drive/My Drive/blip_embeddings/lang_df_{MODEL_}.vec',sep = ' ')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
